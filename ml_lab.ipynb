{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LHVCagHiXvn",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Experiment : 1\n",
        "import random\n",
        "import csv\n",
        "attributes = [[\"Sunny\", \"Rainy\"], [\"Warm\", \"Cold\"], [\"Normal\", \"High\"], [\"Strong\", \"Weak\"], [\"Warm\", \"Cool\"], [\"Same\", \"Change\"]]\n",
        "num_attributes = len(attributes)\n",
        "print(\"The most general hypothesis: ['?', '?', '?', '?', '?', '?']\")\n",
        "print(\"The most general hypothesis: ['0', '0', '0', '0', '0', '0']\")\n",
        "a = []\n",
        "print(\"The given training dataset: \")\n",
        "file_path  = \"C:/Users/kkndc/Downloads/sample_restaurants.restaurants.csv\"\n",
        "with open(file_path, 'r') as csvFile:\n",
        "    reader = csv.reader(csvFile)\n",
        "    for row in reader:\n",
        "        a.append(row)\n",
        "print(row)\n",
        "print(\"The initial value of hypothesis: \")\n",
        "hypothesis = ['0'] * num_attributes\n",
        "print(hypothesis)\n",
        "for j in range(0, num_attributes):\n",
        "    hypothesis[j] = a[0][j]\n",
        "print(\"FIND-S: Finding a Maximality Specific Hypothesis\")\n",
        "for i in range(0, len(a)):\n",
        "    if a[i][num_attributes] == \"yes\":\n",
        "        for j in range(0, num_attributes):\n",
        "            if a[i][j] != hypothesis[j]:\n",
        "                hypothesis[j] = '?'\n",
        "            else:\n",
        "                hypothesis[j] = a[i][j]\n",
        "    print(\"For training example no: {0} the hypothesis is \".format(i), hypothesis)\n",
        "print(\"The Maximally Specific Hypothesis for a given training examples:\")\n",
        "print(hypothesis)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 2\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame(pd.read_csv('/content/enjoysport.csv'))\n",
        "concepts = np.array(data.iloc[:, :-1])\n",
        "target = np.array(data.iloc[:, -1])\n",
        "\n",
        "print(\"Concepts:\")\n",
        "print(concepts)\n",
        "print(\"\\nTarget:\")\n",
        "print(target)\n",
        "\n",
        "def learn(concepts, target):\n",
        "    specific_h = concepts[0].copy()\n",
        "    general_h = [[\"?\" for _ in range(len(specific_h))] for _ in range(len(specific_h))]\n",
        "\n",
        "    print(\"\\nInitialization:\")\n",
        "    print(\"Specific Hypothesis (S):\", specific_h)\n",
        "    print(\"General Hypothesis (G):\", general_h)\n",
        "\n",
        "    for i, h in enumerate(concepts):\n",
        "        if target[i] == \"yes\":\n",
        "            for x in range(len(specific_h)):\n",
        "                if h[x] != specific_h[x]:\n",
        "                    specific_h[x] = '?'\n",
        "                    general_h[x][x] = '?'\n",
        "\n",
        "        elif target[i] == \"no\":\n",
        "            for x in range(len(specific_h)):\n",
        "                if h[x] != specific_h[x]:\n",
        "                    general_h[x][x] = specific_h[x]\n",
        "                else:\n",
        "                    general_h[x][x] = '?'\n",
        "\n",
        "        print(f\"\\nStep {i+1}: Updated Hypotheses\")\n",
        "        print(\"Specific Hypothesis (S):\", specific_h)\n",
        "        print(\"General Hypothesis (G):\", general_h)\n",
        "\n",
        "    general_h = [hyp for hyp in general_h if hyp != ['?'] * len(specific_h)]\n",
        "\n",
        "    return specific_h, general_h\n",
        "\n",
        "s_final, g_final = learn(concepts, target)\n",
        "\n",
        "print(\"\\nFinal Specific Hypothesis (S):\")\n",
        "print(s_final)\n",
        "\n",
        "print(\"\\nFinal General Hypothesis (G):\")\n",
        "print(g_final)\n",
        "\n"
      ],
      "metadata": {
        "id": "rReJIC8ilyOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 3\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "def id3(df, target_attribute_name, attribute_names, default_class=None):\n",
        "    if len(set(df[target_attribute_name])) == 1:\n",
        "        return df[target_attribute_name].iloc[0]\n",
        "    elif len(attribute_names) == 0:\n",
        "        return default_class\n",
        "    else:\n",
        "        gains = {attribute_name: information_gain(df, attribute_name, target_attribute_name) for attribute_name in attribute_names}\n",
        "        best_attribute = max(gains, key=gains.get)\n",
        "        tree = {best_attribute: {}}\n",
        "        remaining_attributes = [attr for attr in attribute_names if attr != best_attribute]\n",
        "\n",
        "        for value in df[best_attribute].unique():\n",
        "            subset = df[df[best_attribute] == value]\n",
        "            subtree = id3(subset, target_attribute_name, remaining_attributes, default_class)\n",
        "            tree[best_attribute][value] = subtree\n",
        "\n",
        "        return tree\n",
        "\n",
        "def entropy(probs):\n",
        "    return sum([-prob * math.log(prob, 2) for prob in probs if prob != 0])\n",
        "\n",
        "def entropy_of_list(a_list):\n",
        "    total_instances = len(a_list)\n",
        "    class_counts = a_list.value_counts()\n",
        "    probs = class_counts / total_instances\n",
        "    return entropy(probs)\n",
        "\n",
        "def information_gain(df, split_attribute_name, target_attribute_name):\n",
        "    total_entropy = entropy_of_list(df[target_attribute_name])\n",
        "    subset_entropy = df.groupby(split_attribute_name)[target_attribute_name].apply(entropy_of_list)\n",
        "    subset_sizes = df.groupby(split_attribute_name).size()\n",
        "    weighted_entropy = (subset_entropy * subset_sizes / len(df)).sum()\n",
        "    return total_entropy - weighted_entropy\n",
        "\n",
        "df = pd.read_csv('/content/id3.csv')\n",
        "\n",
        "attribute_names = list(df.columns)\n",
        "target_attribute_name = 'Answer'\n",
        "attribute_names.remove(target_attribute_name)\n",
        "\n",
        "tree = id3(df, target_attribute_name, attribute_names)\n",
        "\n",
        "print(\"Decision Tree:\")\n",
        "print(tree)\n",
        "\n"
      ],
      "metadata": {
        "id": "eFhtxRULmaZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 4\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "IceCream=pd.read_csv('/content/IceCreamData.csv')\n",
        "print(IceCream)\n",
        "\n",
        "\n",
        "\n",
        "# Divide the data into “Attributes” and “labels”\n",
        "X = IceCream[['Temperature']]\n",
        "y = IceCream['Revenue']\n",
        "# Split 80% of the data to the training set while 20% of the data to test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# Create a Linear Regression model and fit it\n",
        "regressor =LinearRegression(fit_intercept=True)\n",
        "regressor.fit(X_train,y_train)\n",
        "print('Linear Model Coeff (m) =' , regressor.coef_)\n",
        "print('Linear Model Coeff (b) =' , regressor.intercept_)\n",
        "# Predicting the data\n",
        "y_predict=regressor.predict(X_test)\n",
        "print(y_predict)\n",
        "\n",
        "\n",
        "\n",
        "# Scatter plot on Training Data\n",
        "plt.scatter(X_train,y_train,color='blue')\n",
        "plt.plot(X_train,regressor.predict(X_train),color='red')\n",
        "plt.ylabel('Revenue [$]')\n",
        "plt.xlabel('Temperatur [degC]')\n",
        "plt.title('Revenue Generated vs. Temperature @Ice Cream Stand (Training)')\n",
        "\n",
        "\n",
        "# Scatter plot on Testing Data\n",
        "plt.scatter(X_test,y_test,color='blue')\n",
        "plt.plot(X_test,regressor.predict(X_test),color='red')\n",
        "plt.ylabel('Revenue [$]')\n",
        "plt.xlabel('Temperatur [degC]')\n",
        "plt.title('Revenue Generated vs. Temperature @Ice Cream Stand (Training)')\n",
        "\n",
        "\n",
        "\n",
        "# Prediction the revenve using Temperature Value directly\n",
        "print('---------0---------')\n",
        "Temp = -0\n",
        "Revenue = regressor.predict([[Temp]])\n",
        "print(Revenue)\n",
        "print('--------35----------')\n",
        "Temp = 35\n",
        "Revenue = regressor.predict([[Temp]])\n",
        "print(Revenue)\n",
        "print('--------55----------')\n",
        "Temp = 55\n",
        "Revenue = regressor.predict([[Temp]])\n",
        "print(Revenue)\n",
        "\n"
      ],
      "metadata": {
        "id": "ODVg3LxyssqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 5\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "dataset = pd.read_csv('/content/Social_Network_Ads.csv')\n",
        "print(dataset)\n",
        "\n",
        "\n",
        "X = dataset.iloc[:, [2,3]].values\n",
        "y = dataset.iloc[:, 4].values\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression(random_state=0)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "from matplotlib.colors import ListedColormap\n",
        "X_set, y_set = X_train, y_train\n",
        "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
        "np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
        "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
        "alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
        "c = ListedColormap(('red', 'green'))(i), label = j)\n",
        "plt.title('Logistic Regression (Training set)')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Estimated Salary')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "X_set, y_set = X_test, y_test\n",
        "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
        "np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
        "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
        "alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
        "c = ListedColormap(('red', 'green'))(i), label = j)\n",
        "plt.title('Logistic Regression (Test set)')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Estimated Salary')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DbRE_Keyss6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 6\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "import pandas as pd\n",
        "\n",
        "dataset = load_breast_cancer(as_frame=True)\n",
        "X = dataset['data']\n",
        "y = dataset['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "\n",
        "ss_train = StandardScaler()\n",
        "X_train = ss_train.fit_transform(X_train)\n",
        "\n",
        "ss_test = StandardScaler()\n",
        "X_test = ss_test.fit_transform(X_test)\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Support Vector Machines': LinearSVC(),\n",
        "    'Decision Trees': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'K-Nearest Neighbor': KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "accuracy, precision, recall = {}, {}, {}\n",
        "\n",
        "for key in models.keys():\n",
        "    models[key].fit(X_train, y_train)\n",
        "    predictions = models[key].predict(X_test)\n",
        "\n",
        "    accuracy[key] = accuracy_score(y_test, predictions)\n",
        "    precision[key] = precision_score(y_test, predictions)\n",
        "    recall[key] = recall_score(y_test, predictions)\n",
        "\n",
        "df_model = pd.DataFrame(index=models.keys(), columns=['Accuracy', 'Precision', 'Recall'])\n",
        "df_model['Accuracy'] = accuracy.values()\n",
        "df_model['Precision'] = precision.values()\n",
        "df_model['Recall'] = recall.values()\n",
        "\n",
        "ax = df_model.plot.barh()\n",
        "ax.legend(\n",
        "    ncol=len(models.keys()),\n",
        "    bbox_to_anchor=(0, 1),\n",
        "    loc=\"lower left\",\n",
        "    prop={'size': 14}\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test,predictions)\n",
        "TN,FP,FN,TP=confusion_matrix(y_test,predictions).ravel()\n",
        "print(\"True Positive(TP):\",TP)\n",
        "print(\"False Positive(FP):\",FP)\n",
        "print(\"True Negative(TN):\",TN)\n",
        "print(\"False Negative(FN):\",FN)\n",
        "accuracy=(TP+TN)/(TP+FP+TN+FN)\n",
        "print(\"Accuracy of the binary classifier = {:0.3f}\".format(accuracy))"
      ],
      "metadata": {
        "id": "Rh6_TKA-ss9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 7\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris_data = pd.read_csv(\"https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv\")\n",
        "\n",
        "# Remove duplicates\n",
        "iris_data_no_duplicates = iris_data.drop_duplicates()\n",
        "\n",
        "# Split dataset into features and target variable\n",
        "X = iris_data_no_duplicates.drop(columns=['species'])\n",
        "y = iris_data_no_duplicates['species']\n",
        "\n",
        "# Convert categorical labels into numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute Bias (1 - Accuracy)\n",
        "bias = 1 - accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Compute Variance using Cross-Validation\n",
        "cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
        "variance = np.var(cv_scores)\n",
        "\n",
        "# Print results\n",
        "print(\"Bias (1 - Accuracy):\", bias)\n",
        "print(\"Variance (Cross-validation variance):\", variance)\n",
        "print(\"Cross-validation Accuracy:\", cv_scores.mean())\n"
      ],
      "metadata": {
        "id": "oo6m0cfgu1-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 8\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Sample employee data\n",
        "data = {\n",
        "    'Employee id': [10, 20, 15, 25, 30],\n",
        "    'Gender': ['M', 'F', 'F', 'M', 'F'],\n",
        "    'Remarks': ['Good', 'Nice', 'Good', 'Great', 'Nice'],\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "print(f\"Employee data:\\n{df}\\n\")\n",
        "\n",
        "# Extract categorical columns\n",
        "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Initialize OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "# Apply one-hot encoding\n",
        "one_hot_encoded = encoder.fit_transform(df[categorical_columns])\n",
        "\n",
        "# Create DataFrame with encoded columns\n",
        "one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Concatenate original and encoded data\n",
        "df_encoded = pd.concat([df, one_hot_df], axis=1)\n",
        "\n",
        "# Drop original categorical columns\n",
        "df_encoded = df_encoded.drop(categorical_columns, axis=1)\n",
        "\n",
        "# Print encoded data\n",
        "print(f\"Encoded Employee data:\\n{df_encoded}\")\n"
      ],
      "metadata": {
        "id": "zYNSYeauu2BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 9\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Color': ['Red', 'Blue', 'Green', 'Red', 'Green'],\n",
        "    'Size': ['Small', 'Large', 'Medium', 'Medium', 'Small'],\n",
        "    'Shape': ['Circle', 'Square', 'Triangle', 'Circle', 'Square'],\n",
        "    'Label': ['A', 'B', 'C', 'A', 'B']\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Original dataset:\")\n",
        "print(df)\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df_encoded = df.copy()\n",
        "\n",
        "# Apply Label Encoding to categorical columns\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':  # Ensure encoding is applied only to categorical columns\n",
        "        df_encoded[col] = label_encoder.fit_transform(df[col])\n",
        "\n",
        "print(\"\\nAfter Label Encoding:\")\n",
        "print(df_encoded)\n"
      ],
      "metadata": {
        "id": "y_A4nRTpu2Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 10\n",
        "import numpy as np\n",
        "\n",
        "# Input and output datasets\n",
        "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n",
        "y = np.array(([92], [86], [89]), dtype=float)\n",
        "\n",
        "# Feature scaling (Normalizing inputs)\n",
        "X = X / np.amax(X, axis=0)  # Normalize X\n",
        "y = y / 100  # Normalize y\n",
        "\n",
        "# Activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def derivatives_sigmoid(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Neural Network parameters\n",
        "epoch = 5000      # Number of iterations\n",
        "lr = 0.1          # Learning rate\n",
        "input_neurons = 2  # Number of input neurons\n",
        "hidden_neurons = 3 # Number of hidden neurons\n",
        "output_neurons = 1 # Number of output neurons\n",
        "\n",
        "# Weight and bias initialization (random values)\n",
        "wh = np.random.uniform(size=(input_neurons, hidden_neurons))\n",
        "bh = np.random.uniform(size=(1, hidden_neurons))\n",
        "wout = np.random.uniform(size=(hidden_neurons, output_neurons))\n",
        "bout = np.random.uniform(size=(1, output_neurons))\n",
        "\n",
        "# Training process\n",
        "for i in range(epoch):\n",
        "    # Forward Propagation\n",
        "    hidden_layer_input = np.dot(X, wh) + bh\n",
        "    hidden_layer_activation = sigmoid(hidden_layer_input)\n",
        "\n",
        "    output_layer_input = np.dot(hidden_layer_activation, wout) + bout\n",
        "    output = sigmoid(output_layer_input)\n",
        "\n",
        "    # Backpropagation\n",
        "    error_output = y - output\n",
        "    output_gradient = derivatives_sigmoid(output)\n",
        "    d_output = error_output * output_gradient\n",
        "\n",
        "    error_hidden = d_output.dot(wout.T)\n",
        "    hidden_gradient = derivatives_sigmoid(hidden_layer_activation)\n",
        "    d_hidden = error_hidden * hidden_gradient\n",
        "\n",
        "    # Updating Weights and Biases\n",
        "    wout += hidden_layer_activation.T.dot(d_output) * lr\n",
        "    wh += X.T.dot(d_hidden) * lr\n",
        "\n",
        "# Final output\n",
        "print(\"Input: \\n\", X)\n",
        "print(\"Actual Output: \\n\", y)\n",
        "print(\"Predicted Output: \\n\", output)\n"
      ],
      "metadata": {
        "id": "PFnhHrJ_u2JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 11\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Splitting dataset into training and testing sets (10% test data)\n",
        "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1, random_state=42)\n",
        "\n",
        "# Displaying label names\n",
        "print(\"Iris Flower Labels:\")\n",
        "for i in range(len(iris.target_names)):\n",
        "    print(f\"Label {i} - {iris.target_names[i]}\")\n",
        "\n",
        "# K-NN Classifier (Using K=2)\n",
        "k = 2  # Number of neighbors\n",
        "classifier = KNeighborsClassifier(n_neighbors=k)\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# Predicting the test set\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "# Displaying Classification Results\n",
        "print(f\"\\nResults of Classification using K-NN with K={k}:\")\n",
        "for i in range(len(x_test)):\n",
        "    print(f\"Sample: {x_test[i]} | Actual Label: {y_test[i]} ({iris.target_names[y_test[i]]}) | \"\n",
        "          f\"Predicted Label: {y_pred[i]} ({iris.target_names[y_pred[i]]})\")\n",
        "\n",
        "# Classification Accuracy\n",
        "accuracy = classifier.score(x_test, y_test)\n",
        "print(f\"\\nClassification Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "ZuIZPko3u2L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 12\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def lowess(x, y, f, iterations):\n",
        "    n = len(x)\n",
        "    r = int(ceil(f * n))  # Number of nearest neighbors\n",
        "    h = [np.sort(np.abs(x - x[i]))[r] for i in range(n)]\n",
        "\n",
        "    # Compute weights\n",
        "    w = np.clip(np.abs((x[:, None] - x[None, :]) / h), 0.0, 1.0)\n",
        "    w = (1 - w ** 3) ** 3  # Apply tricube weight function\n",
        "\n",
        "    yest = np.zeros(n)  # Smoothed values\n",
        "    delta = np.ones(n)  # Robustness weights\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        for i in range(n):\n",
        "            weights = delta * w[:, i]\n",
        "            b = np.array([np.sum(weights * y), np.sum(weights * y * x)])\n",
        "            A = np.array([\n",
        "                [np.sum(weights), np.sum(weights * x)],\n",
        "                [np.sum(weights * x), np.sum(weights * x * x)]\n",
        "            ])\n",
        "            beta = np.linalg.solve(A, b)  # Solve for beta\n",
        "            yest[i] = beta[0] + beta[1] * x[i]\n",
        "\n",
        "        residuals = y - yest\n",
        "        s = np.median(np.abs(residuals))  # Median absolute deviation\n",
        "        delta = np.clip(residuals / (6.0 * s), -1, 1)\n",
        "        delta = (1 - delta ** 2) ** 2  # Apply robustness function\n",
        "\n",
        "    return yest\n",
        "\n",
        "# Generate sample data\n",
        "n = 100\n",
        "x = np.linspace(0, 2 * np.pi, n)\n",
        "y = np.sin(x) + 0.3 * np.random.randn(n)  # Add noise\n",
        "\n",
        "# Apply LOWESS smoothing\n",
        "f = 0.25  # Smoothing factor\n",
        "iterations = 3\n",
        "yest = lowess(x, y, f, iterations)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(x, y, color='red', label=\"Noisy Data\", alpha=0.6)\n",
        "plt.plot(x, yest, color='blue', linewidth=2, label=\"LOWESS Smoothed Curve\")\n",
        "plt.xlabel(\"X values\")\n",
        "plt.ylabel(\"Y values\")\n",
        "plt.title(\"LOWESS Smoothing on Noisy Sinusoidal Data\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Hs0iIFQostAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rGdfCZCqstDO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}